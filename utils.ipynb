{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of users and passwords registered\n",
    "AUTHORIZED_USERS = {\n",
    "    \"Arianna\": \"falcon\",\n",
    "    \"Andre\": \"tiger\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.spatial.distance import cosine\n",
    "from speechbrain.inference import SpeakerRecognition\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice acquisition module\n",
    "The user speaks into the microphone.\n",
    "The signal is preprocessed to remove noise and normalized.\n",
    "\n",
    "Alternatively, one of the audio files already present in the folder can be used.\n",
    "The signal is processed in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(signal, sr, low_cutoff=100, high_cutoff=3000):\n",
    "    normalized_signal = librosa.util.normalize(signal)\n",
    "    nyquist = 0.5 * sr\n",
    "    low = low_cutoff / nyquist\n",
    "    high = high_cutoff / nyquist\n",
    "    b, a = butter(5, [low, high], btype='band')\n",
    "\n",
    "    return lfilter(b, a, normalized_signal)\n",
    "\n",
    "# function to register audio in real time\n",
    "def acquire_audio_new():\n",
    "    sr = 16000  # sampling rate\n",
    "    duration = 4\n",
    "    print(\"Recording audio...\")\n",
    "    \n",
    "    audio = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')\n",
    "    sd.wait()  # wait the end of registration\n",
    "\n",
    "    print(\"Audio recorded. Preprocessing...\")\n",
    "    processed_audio = preprocess_audio(audio.flatten(), sr)\n",
    "    print(\"Audio preprocessed.\")\n",
    "    return processed_audio, sr\n",
    "\n",
    "# function to acquire audio from \"test\" folder\"\n",
    "def acquire_audio_from_test(n):\n",
    "    test_folder = \"./dataset/test\" \n",
    "    audio_files = [f for f in os.listdir(test_folder) if f.endswith('.wav')]\n",
    "\n",
    "    if not audio_files:\n",
    "        raise FileNotFoundError(\"No audio files found in the 'test' folder.\")\n",
    "\n",
    "    if n == 0:  # Se n == 0, random choice\n",
    "        selected_file = random.choice(audio_files)\n",
    "    else:  # otherwise, choose n-th file\n",
    "        if n > len(audio_files):\n",
    "            raise IndexError(f\"The test folder contains only {len(audio_files)} files.\")\n",
    "        audio_files.reverse()\n",
    "        selected_file = audio_files[n - 1]\n",
    "\n",
    "    file_path = os.path.join(test_folder, selected_file)\n",
    "    print(f\"Loading audio file: {selected_file}\")\n",
    "    signal, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    print(\"Preprocessing audio...\")\n",
    "    processed_audio = preprocess_audio(signal, sr)\n",
    "    print(\"Audio preprocessed.\")\n",
    "    return processed_audio, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker identification\n",
    "\n",
    "Generates a unique numerical embedding from voice characteristics to identify the speaker. \n",
    "\n",
    "Returns the estimated identity (\"Arianna\", \"Andre\", or \"Unauthorized\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/ariannapierini/Library/Python/3.9/lib/python/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Users/ariannapierini/Library/Python/3.9/lib/python/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "recognizer = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"./model\")\n",
    "\n",
    "# function to create authorized speaker embeddings\n",
    "def create_speaker_embedding(audio_paths):\n",
    "    embeddings = []\n",
    "    \n",
    "    for audio_path in audio_paths:\n",
    "        signal, sr = librosa.load(audio_path, sr=None)\n",
    "        processed_audio = preprocess_audio(signal, sr)\n",
    "        embedding = recognizer.encode_batch(torch.tensor([processed_audio]))\n",
    "        embeddings.append(embedding.flatten().detach().numpy())\n",
    "\n",
    "    mean_embedding = np.mean(embeddings, axis=0)\n",
    "    return mean_embedding\n",
    "\n",
    "# function to generate sentence file paths\n",
    "def generate_sentence_file_paths(base_name, num_files):\n",
    "    return [f\"dataset/registered/sentences/{base_name}_pw{i}.wav\" for i in range(1, num_files + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arianna embeddings: [ 35.52124     21.893566    22.68876    -21.29689      6.066318\n",
      "  26.263565     2.7747447    6.102174     0.26386526  24.066471\n",
      "   2.9939454    2.250876     9.40031      4.4074373   15.027156\n",
      "  -6.3589296   34.96337     25.187433     6.101095     8.3119545\n",
      "  25.3614      27.620804    17.98833     18.622684    18.084957\n",
      " -14.810003   -32.013073   -11.776002    26.108643   -12.655676\n",
      "   5.726528   -10.453799    -4.306484   -16.427935   -28.083847\n",
      "  -7.281644     1.3660741   21.933296    -0.06710432  38.876648\n",
      "   7.875436     6.077498    21.915012   -20.998077    24.029057\n",
      "   0.53644764 -19.099873     3.9147706   22.783436    -8.539175\n",
      "  19.578297    -7.700962   -18.176714    25.095062    -7.23887\n",
      "   2.9536932  -32.201027     0.8318567  -10.680226    13.25445\n",
      "  36.97503     -4.4166102  -25.694834    -5.250004    -9.53453\n",
      "   5.4720173   16.373264   -10.620026     6.1696253   -7.76967\n",
      "  15.917302    21.576717    -4.0674734  -20.908527     1.2099098\n",
      "  16.609087   -23.877995    25.979282    -3.696776    -1.6149092\n",
      "  28.15661     28.653214    16.602127   -25.25583    -27.418415\n",
      "  41.1219       9.22467      6.2950773  -15.533763    14.6805315\n",
      " -26.955524   -25.261333   -32.370083    37.21656     -9.038729\n",
      " -21.262035    13.398181    -4.7914357   17.815218    11.557184\n",
      " -17.640892    13.558851     3.826805    -1.7428811    9.436888\n",
      " -21.099936    44.007374     3.917005     3.2995644   14.4787035\n",
      " -26.406427    29.469614   -10.124876     3.246386   -15.944445\n",
      " -20.390566    -5.3751698  -45.934887     7.882347   -42.28369\n",
      "  14.792056     9.302444    -8.453688   -36.504883    23.030483\n",
      "  -8.465995   -26.02827    -14.602394   -14.854506    39.46856\n",
      "  14.290309     4.5602746  -27.038235   -15.429794     8.51862\n",
      "  -3.6469598    6.2026777   -8.972846    10.191841   -20.263935\n",
      "  -2.633479    -0.4970892   22.880817    -1.9304075    6.7486954\n",
      " -12.060736   -13.774608     3.9450805   -1.6283957   -0.9638033\n",
      "  -7.383246   -15.329605   -22.38055    -17.944069    31.508968\n",
      "  -7.6938033  -19.925968    18.426264   -20.500067     7.287757\n",
      "  -8.224781     3.0684004   -5.5871367   -0.4733773    3.8398013\n",
      " -17.093315     1.5577321    9.186545     3.1537733   13.5255375\n",
      "  11.889728   -25.965153   -26.864166    -6.473748    -2.2477064\n",
      " -12.53702     33.025047    30.299992    54.68991     -2.6382022\n",
      "  16.000017    21.188528   -24.314001    21.143892    20.889093\n",
      "  12.56754      2.0751114   -6.3691673   17.893608    37.243607\n",
      " -13.672801    -8.661765  ]\n",
      "andre embeddings: [ 33.538704    20.108555    12.013365   -10.424616    -1.6343571\n",
      "  27.942993     9.952388     4.608403     6.6571436   11.269819\n",
      "  -7.107579    -5.1083426    7.043322     7.062275    14.86243\n",
      "   1.0778651   42.752373    19.123362    17.928125     2.0671277\n",
      "  12.02493     18.696318    19.621542    22.493336     8.526167\n",
      " -16.310585   -27.209034    -0.41465068  28.22911     -0.27239555\n",
      "  10.947846    12.848178     0.27339172  -1.0458462  -24.740295\n",
      "  -8.650401    24.586561    20.150064    -1.1920878   45.035606\n",
      "  -5.473667    -5.759899    25.174963   -28.930569    19.74003\n",
      "  -3.443959    -6.7446227   20.937817     9.013925     8.604098\n",
      "  15.856543   -16.698277   -14.069432    17.062098   -11.471726\n",
      "   8.449745   -31.627869    13.221962   -16.925217     3.1587892\n",
      "   8.946225   -25.778736   -16.90484    -23.615004     1.0722263\n",
      "  13.756502    23.946419     0.5660902    1.3936995   -4.0918527\n",
      "  12.61801      8.247589   -10.344989   -11.058658   -11.366404\n",
      "  17.410173   -12.729673    29.02039     -5.6539645   12.291782\n",
      "  28.045319    23.161808     7.597225   -33.213814   -21.165754\n",
      "  33.17977      4.4348364    6.488353   -19.67214      7.593796\n",
      " -16.93898    -15.9083605  -39.029305    30.607647   -11.293184\n",
      " -25.269299    12.865336     5.8317213    4.8476076   11.409417\n",
      "  -2.402201    17.395279     2.138419     5.6820188   14.739321\n",
      " -16.000978    34.810204    -3.0880244  -13.462372     8.696627\n",
      " -35.352505    32.668465   -15.889195    -5.4902997  -29.464417\n",
      " -12.763614     1.6157081  -32.12605    -13.031752   -25.730082\n",
      "  21.16566     13.876762    -8.016426   -44.00522     31.820398\n",
      " -15.299357   -12.8217945    2.3405716   -2.6934028   25.962528\n",
      "  10.766373     4.6960683  -16.682613   -10.34307      3.6561756\n",
      "  -6.615316    13.560887    -9.819329    23.92257     -4.467885\n",
      "  -3.84743     -4.6416693   34.98899      1.2673701    4.5975327\n",
      "  -5.124205   -14.1034565    4.339553    -0.99703676  14.921389\n",
      " -10.414109    -6.994647     0.13327034 -33.74493     30.146606\n",
      "  -1.9054058  -20.49624     14.33568    -30.037521     3.5789132\n",
      "  -0.49704576  10.11755     -2.091516     9.265002    17.742613\n",
      " -16.969612    10.017188    16.929659   -10.768129    10.366018\n",
      "   3.831208   -23.15534    -12.059488    -5.813121     2.513451\n",
      " -22.10075     26.078465    28.851797    31.468771   -19.977566\n",
      "   5.6550226   16.746883   -16.232822    18.747053    32.88883\n",
      "  12.76342      9.497299    -5.4982977   11.030968    27.707973\n",
      "  -3.6225529   -1.0726455 ]\n",
      "Embeddings created for authorized people\n"
     ]
    }
   ],
   "source": [
    "arianna_sentence_files = generate_sentence_file_paths('arianna', 20)\n",
    "andre_sentence_files = generate_sentence_file_paths('andre', 20) \n",
    "\n",
    "arianna_embedding = create_speaker_embedding(arianna_sentence_files)\n",
    "print(f\"arianna embeddings: {arianna_embedding}\")\n",
    "andre_embedding = create_speaker_embedding(andre_sentence_files)\n",
    "print(f\"andre embeddings: {andre_embedding}\")\n",
    "\n",
    "torch.save({\"Arianna\": arianna_embedding, \"Andre\": andre_embedding}, \"speakers_embeddings.pth\")\n",
    "print(\"Embeddings created for authorized people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compare test audio with registered speakers\n",
    "def verify_speaker(processed_audio, speakers_embeddings, threshold=0.7):\n",
    "    test_embedding = recognizer.encode_batch(torch.tensor([processed_audio])) \n",
    "    test_embedding = test_embedding.squeeze()  \n",
    "    test_embedding = test_embedding / torch.norm(test_embedding)\n",
    "\n",
    "    scores = {}\n",
    "    for speaker, reference_embedding in speakers_embeddings.items():\n",
    "        reference_embedding = reference_embedding / torch.norm(torch.tensor([reference_embedding]))\n",
    "        similarity_score = 1 - cosine(test_embedding, reference_embedding)\n",
    "        scores[speaker] = similarity_score\n",
    "\n",
    "    recognized_speaker = max(scores, key=scores.get)\n",
    "    if scores[recognized_speaker] >= threshold: \n",
    "        return recognized_speaker, scores\n",
    "    else:\n",
    "        return \"Unauthorized\", scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Password Verification\n",
    "\n",
    "The system checks that the person is using their own password and not someone else's. The spoken password is transcribed into text and compared with the registered password of the identified user. To prevent transcription errors, the system also performs a direct audio verification, comparing the spoken sounds with the stored acoustic model of the password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password embeddings created for authorized people\n"
     ]
    }
   ],
   "source": [
    "# function to generate password file paths\n",
    "def generate_password_file_paths(base_name, num_files):\n",
    "    return [f\"dataset/registered/passwords/{base_name}_pw{i}.wav\" for i in range(1, num_files + 1)]\n",
    "\n",
    "arianna_pw_files = generate_password_file_paths('arianna', 20)\n",
    "andre_pw_files = generate_password_file_paths('andre', 20) \n",
    "\n",
    "arianna_pw_embedding = create_speaker_embedding(arianna_pw_files)\n",
    "andre_pw_embedding = create_speaker_embedding(andre_pw_files)\n",
    "\n",
    "torch.save({\"Arianna\": arianna_pw_embedding, \"Andre\": andre_pw_embedding}, \"password_embeddings.pth\")\n",
    "print(\"Password embeddings created for authorized people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_password(audio_test, user, password_embeddings, threshold=0.6):\n",
    "    user_embedding = password_embeddings.get(user)\n",
    "    user_embedding = user_embedding / torch.norm(torch.tensor([user_embedding]))\n",
    "\n",
    "    # verify through audio comparing\n",
    "    test_embedding = recognizer.encode_batch(torch.tensor([audio_test]))\n",
    "    test_embedding = test_embedding.squeeze()  \n",
    "    test_embedding = test_embedding / torch.norm(test_embedding)\n",
    "    similarity_score = 1 - cosine(test_embedding, user_embedding)\n",
    "\n",
    "    audio_match = similarity_score >= threshold\n",
    "    \n",
    "    return audio_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision\n",
    "\n",
    "If both the speaker and the password are correct, access is granted. Otherwise, access is denied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_access(audio_test, user, password_embeddings):\n",
    "    if verify_password(audio_test, user, password_embeddings):\n",
    "        print(\"Access allowed!\")\n",
    "    else:\n",
    "        print(\"Access denied.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: test_andre_falcon2.wav\n",
      "Preprocessing audio...\n",
      "Audio preprocessed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/95bqt2d14s56nk202mq58vrh0000gn/T/ipykernel_45524/603103377.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  speakers_embeddings = torch.load(\"speakers_embeddings.pth\")\n",
      "/var/folders/sr/95bqt2d14s56nk202mq58vrh0000gn/T/ipykernel_45524/603103377.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  password_embeddings = torch.load(\"password_embeddings.pth\")\n"
     ]
    }
   ],
   "source": [
    "speakers_embeddings = torch.load(\"speakers_embeddings.pth\")\n",
    "password_embeddings = torch.load(\"password_embeddings.pth\")\n",
    "\n",
    "# --- Test audio loading\n",
    "# [processed_audio, sr] = acquire_audio_new()\n",
    "[processed_audio, sr] = acquire_audio_from_test(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified: ('Andre', {'Arianna': np.float32(0.7714257), 'Andre': np.float32(0.8707419)})\n",
      "The test speaker is a registered user!\n",
      "Access allowed!\n"
     ]
    }
   ],
   "source": [
    "# --- First step: verify if the test speaker is a registered one\n",
    "result = verify_speaker(processed_audio, speakers_embeddings)\n",
    "print(f\"Identified: {result}\")\n",
    "\n",
    "if result[0] == \"Unauthorized\":\n",
    "    print(\"The test speaker is not registered among the authorized users...bye.\")\n",
    "    raise SystemExit()\n",
    "else:\n",
    "    print(\"The test speaker is a registered user!\")\n",
    "    # --- Second step (only if the user is registered): verify if the password is correct\n",
    "    manage_access(processed_audio, result[0], password_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
